{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fdafda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "max_size = max([sys.getsizeof(record) for record in df_dict])\n",
    "print(f\"Maximum document size: {max_size} bytes\")\n",
    "total_size = sum([sys.getsizeof(record) for record in df_dict])\n",
    "print(f\"Total size of all documents: {total_size} bytes\")\n",
    "print(f\"Total size of all documents in MB: {(total_size)/(1048576)} MB\")\n",
    "##1 mb in bytes\n",
    "##1 MB = 1,048,576 bytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15f282da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st=4==8\n",
    "st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8ae696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from src.constants import TARGET_COLUMN, SCHEMA_FILE_PATH, CURRENT_YEAR\n",
    "from src.entity.config_entity import DataTransformationConfig\n",
    "from src.entity.artifact_entity import DataTransformationArtifact, DataIngestionArtifact, DataValidationArtifact\n",
    "from src.exception import MyException\n",
    "from src.logger import logging\n",
    "from src.utils.main_utils import save_object, save_numpy_array_data, read_yaml_file\n",
    "\n",
    "\n",
    "class LabelEncodingTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Custom transformer to apply LabelEncoder to categorical columns.\n",
    "    \"\"\"\n",
    "    def __init__(self, categorical_columns):\n",
    "        self.categorical_columns = categorical_columns\n",
    "        self.label_encoders = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        for col in self.categorical_columns:\n",
    "            le = LabelEncoder()\n",
    "            le.fit(X[col])\n",
    "            self.label_encoders[col] = le\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for col in self.categorical_columns:\n",
    "            X[col] = self.label_encoders[col].transform(X[col])\n",
    "        return X\n",
    "\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, data_ingestion_artifact: DataIngestionArtifact,\n",
    "                 data_transformation_config: DataTransformationConfig,\n",
    "                 data_validation_artifact: DataValidationArtifact):\n",
    "        try:\n",
    "            self.data_ingestion_artifact = data_ingestion_artifact\n",
    "            self.data_transformation_config = data_transformation_config\n",
    "            self.data_validation_artifact = data_validation_artifact\n",
    "            self._schema_config = read_yaml_file(file_path=SCHEMA_FILE_PATH)\n",
    "        except Exception as e:\n",
    "            raise MyException(e, sys)\n",
    "\n",
    "    @staticmethod\n",
    "    def read_data(file_path) -> pd.DataFrame:\n",
    "        try:\n",
    "            return pd.read_csv(file_path)\n",
    "        except Exception as e:\n",
    "            raise MyException(e, sys)\n",
    "\n",
    "    def get_data_transformer_object(self) -> Pipeline:\n",
    "        \"\"\"\n",
    "        Creates and returns a data transformer object for the data, \n",
    "        including gender mapping, dummy variable creation, column renaming,\n",
    "        feature scaling, and type adjustments.\n",
    "        \"\"\"\n",
    "        logging.info(\"Entered get_data_transformer_object method of DataTransformation class\")\n",
    "\n",
    "        try:\n",
    "            # Initialize transformers\n",
    "            numeric_transformer = StandardScaler()\n",
    "            cat_features = self._schema_config['categorical_columns']\n",
    "            num_features = self._schema_config['num_features']\n",
    "\n",
    "            # Custom LabelEncoder transformer\n",
    "            label_encoder_transformer = LabelEncodingTransformer(categorical_columns=cat_features)\n",
    "\n",
    "            # Creating preprocessor pipeline\n",
    "            preprocessor = Pipeline(steps=[\n",
    "                (\"LabelEncoding\", label_encoder_transformer),\n",
    "                (\"StandardScaler\", ColumnTransformer(\n",
    "                    transformers=[\n",
    "                        (\"StandardScaler\", numeric_transformer, num_features)\n",
    "                    ],\n",
    "                    remainder='passthrough'  # Leaves other columns as they are\n",
    "                ))\n",
    "            ])\n",
    "            logging.info(\"Final Pipeline Ready!!\")\n",
    "            logging.info(\"Exited get_data_transformer_object method of DataTransformation class\")\n",
    "            return preprocessor\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.exception(\"Exception occurred in get_data_transformer_object method of DataTransformation class\")\n",
    "            raise MyException(e, sys) from e\n",
    "\n",
    "    def _drop_id_column(self, df):\n",
    "        \"\"\"Drop the 'id' column if it exists.\"\"\"\n",
    "        logging.info(\"Dropping 'id' column\")\n",
    "        drop_col = self._schema_config['drop_columns']\n",
    "        if drop_col in df.columns:\n",
    "            df = df.drop(drop_col, axis=1)\n",
    "        return df\n",
    "\n",
    "    def initiate_data_transformation(self) -> DataTransformationArtifact:\n",
    "        \"\"\"\n",
    "        Initiates the data transformation component for the pipeline.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(\"Data Transformation Started !!!\")\n",
    "            if not self.data_validation_artifact.validation_status:\n",
    "                raise Exception(self.data_validation_artifact.message)\n",
    "\n",
    "            # Load train and test data\n",
    "            train_df = self.read_data(file_path=self.data_ingestion_artifact.trained_file_path)\n",
    "            test_df = self.read_data(file_path=self.data_ingestion_artifact.test_file_path)\n",
    "            logging.info(\"Train-Test data loaded\")\n",
    "\n",
    "            input_feature_train_df = train_df.drop(columns=[TARGET_COLUMN], axis=1)\n",
    "            target_feature_train_df = train_df[TARGET_COLUMN]\n",
    "\n",
    "            input_feature_test_df = test_df.drop(columns=[TARGET_COLUMN], axis=1)\n",
    "            target_feature_test_df = test_df[TARGET_COLUMN]\n",
    "            logging.info(\"Input and Target cols defined for both train and test df.\")\n",
    "\n",
    "            # Apply custom transformations in specified sequence\n",
    "            input_feature_train_df = self._drop_id_column(input_feature_train_df)\n",
    "            input_feature_test_df = self._drop_id_column(input_feature_test_df)\n",
    "            logging.info(\"Custom transformations applied to train and test data\")\n",
    "\n",
    "            logging.info(\"Starting data transformation\")\n",
    "            preprocessor = self.get_data_transformer_object()\n",
    "            logging.info(\"Got the preprocessor object\")\n",
    "\n",
    "            logging.info(\"Initializing transformation for Training-data\")\n",
    "            input_feature_train_arr = preprocessor.fit_transform(input_feature_train_df)\n",
    "            logging.info(\"Initializing transformation for Testing-data\")\n",
    "            input_feature_test_arr = preprocessor.transform(input_feature_test_df)\n",
    "            logging.info(\"Transformation done end to end to train-test df.\")\n",
    "\n",
    "            logging.info(\"Applying SMOTEENN for handling imbalanced dataset.\")\n",
    "            smt = SMOTEENN(sampling_strategy=\"minority\")\n",
    "            input_feature_train_final, target_feature_train_final = smt.fit_resample(\n",
    "                input_feature_train_arr, target_feature_train_df\n",
    "            )\n",
    "            input_feature_test_final, target_feature_test_final = smt.fit_resample(\n",
    "                input_feature_test_arr, target_feature_test_df\n",
    "            )\n",
    "            logging.info(\"SMOTEENN applied to train-test df.\")\n",
    "\n",
    "            train_arr = np.c_[input_feature_train_final, np.array(target_feature_train_final)]\n",
    "            test_arr = np.c_[input_feature_test_final, np.array(target_feature_test_final)]\n",
    "            logging.info(\"feature-target concatenation done for train-test df.\")\n",
    "\n",
    "            save_object(self.data_transformation_config.transformed_object_file_path, preprocessor)\n",
    "            save_numpy_array_data(self.data_transformation_config.transformed_train_file_path, array=train_arr)\n",
    "            save_numpy_array_data(self.data_transformation_config.transformed_test_file_path, array=test_arr)\n",
    "            logging.info(\"Saving transformation object and transformed files.\")\n",
    "\n",
    "            logging.info(\"Data transformation completed successfully\")\n",
    "            return DataTransformationArtifact(\n",
    "                transformed_object_file_path=self.data_transformation_config.transformed_object_file_path,\n",
    "                transformed_train_file_path=self.data_transformation_config.transformed_train_file_path,\n",
    "                transformed_test_file_path=self.data_transformation_config.transformed_test_file_path\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            raise MyException(e, sys) from e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

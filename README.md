# ğŸ§  Claim Prediction ML Pipeline - End-to-End ML & MLOps Project

Welcome to the **Claim Prediction ML Pipeline**, an end-to-end machine learning project that showcases the implementation of a full MLOps lifecycle, integrating robust backend engineering, data pipeline automation, model training, evaluation, and seamless deployment using AWS services and CI/CD workflows.

---

## ğŸš€ Project Highlights

- ğŸ“¦ **Modular Code Architecture** with reusable components
- ğŸ“Š **MongoDB Atlas** for cloud-based data storage
- ğŸ§¹ **Data Ingestion, Validation & Transformation Pipelines**
- ğŸ¤– **Custom Model Training & Evaluation Logic**
- â˜ï¸ **AWS Integration (S3, IAM, EC2, ECR)**
- ğŸ³ **Docker Containerization**
- âš™ï¸ **CI/CD Workflow** via GitHub Actions + Self-Hosted Runner on EC2
- ğŸŒ **FastAPI Web App Deployment** (port 5000)

---

## ğŸ› ï¸ Project Setup

### 1. ğŸ”§ Project Initialization
```bash
# Run project template generator
python template.py

2. ğŸ“š Package Structure & Setup
	â€¢	Add local packages to setup.py and pyproject.toml

3. ğŸ Virtual Environment

python -m venv inclaim
source inclaim/bin/activate
pip install -r requirements.txt
pip list   # Ensure local packages are installed



â¸»

ğŸ“¦ MongoDB Atlas Setup (Cloud NoSQL)
	1.	Sign up & create MongoDB Atlas cluster (M4 - free tier)
	2.	Create DB user & allow access from anywhere (0.0.0.0/0)
	3.	Copy connection string for Python (v3.6+)
	4.	Add mongoDB_demo.ipynb to notebook/ and push sample data to MongoDB
	5.	Confirm upload via MongoDB Atlas â†’ Database â†’ Browse Collection

â¸»

ğŸ§¾ Logging, Exception Handling & EDA
	â€¢	Custom logger.py and exception.py tested on demo.py
	â€¢	Initial Exploratory Data Analysis and Feature Engineering notebooks provided

â¸»

ğŸ› ï¸ Data Engineering Workflow

ğŸ”„ Data Ingestion
	â€¢	Define MongoDB connection logic in configuration.mongo_db_connections.py
	â€¢	Implement data fetch and transformation in data_access.proj1_data
	â€¢	Structure config and artifact classes in:
	â€¢	entity/config_entity.py
	â€¢	entity/artifact_entity.py
	â€¢	Implement component in components.data_ingestion.py
	â€¢	Run end-to-end from demo.py

ğŸŒ MongoDB URL Setup

# Bash
export MONGODB_URL="mongodb+srv://<username>:<password>@cluster.mongodb.net/"
echo $MONGODB_URL



â¸»

âœ… Data Validation & Transformation
	â€¢	Define schema in config/schema.yaml
	â€¢	Add validation logic in components/data_validation.py
	â€¢	Add transformation logic and estimator in components/data_transformation.py

â¸»

ğŸ¤– Model Training
	â€¢	Implement training logic in components/model_trainer.py
	â€¢	Add model wrapper to entity/estimator.py

â¸»

â˜ï¸ AWS Integration (Model Evaluation & Deployment)

AWS Setup
	â€¢	Create IAM user (firstproj) with AdminAccess
	â€¢	Generate & download access keys
	â€¢	Export as environment variables:

export AWS_ACCESS_KEY_ID="your_key"
export AWS_SECRET_ACCESS_KEY="your_secret"

AWS S3 Configuration
	â€¢	Create bucket claimproj (Region: eu-north-1a)
	â€¢	Set S3 access in constants/__init__.py
	â€¢	Implement logic in:
	â€¢	src/aws_storage
	â€¢	entity/s3_estimator.py
	â€¢	configuration/aws_connection.py

â¸»

ğŸ“ˆ Model Evaluation & Pusher
	â€¢	Compare models using threshold score
	â€¢	Push best model to S3
	â€¢	Setup prediction service using FastAPI/Flask (app.py)

â¸»

ğŸ³ Docker + ğŸ› ï¸ CI/CD Pipeline

Docker
	â€¢	Create Dockerfile and .dockerignore

GitHub Actions + EC2 Self-Hosted Runner
	1.	Connect EC2 to GitHub via runner
	2.	Setup GitHub Secrets:
	â€¢	AWS_ACCESS_KEY_ID
	â€¢	AWS_SECRET_ACCESS_KEY
	â€¢	AWS_DEFAULT_REGION
	â€¢	ECR_REPO
	3.	Create ECR repository (claimproject)
	4.	Configure .github/workflows/aws.yaml

â¸»

ğŸš¢ Deployment
	â€¢	Run pipeline â†’ Docker builds & pushes to ECR
	â€¢	EC2 instance serves model on port 5000
	â€¢	Add 5000 port in AWS security group for public access

# Access App
http://<your-ec2-public-ip>:5000
Sorry i remove the instance from EC2 instance for docker image.


â¸»

ğŸ” Re-Train Model

Use /training route to trigger model retraining on new data.

â¸»

ğŸ“ Conclusion

This project reflects production-ready ML pipeline practices:
	â€¢	Real-world data integration
	â€¢	Scalable architecture
	â€¢	CI/CD automation
	â€¢	Cloud-first deployment mindset

ğŸ’¼ Looking to connect with teams solving impactful ML problems!

â¸»

ğŸ‘¨â€ğŸ’» Author

Saroj: [Data Analyst | ML Engineer]
Feel free to connect on LinkedIn